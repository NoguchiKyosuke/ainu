{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47bc615",
   "metadata": {},
   "source": [
    "# Ainu Speech: Recorded Audio vs. Sample Set\n",
    "\n",
    "This notebook analyzes recorded audio and compares it against a sample Ainu audio set using MFCCs, mel features, DTW-based similarity, and optional pretrained speech embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e8c343",
   "metadata": {},
   "source": [
    "## 1) Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fac5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in a fresh environment, uncomment and run this cell once\n",
    "# !pip install -r ../requirements.txt\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# Local utilities\n",
    "import sys\n",
    "ROOT = Path(\"..\").resolve()\n",
    "sys.path.append(str((ROOT / 'src').resolve()))\n",
    "from audio_utils import load_audio, trim_silence, extract_features, zscore, dtw_distance, cosine_similarity_matrix, compare_recording_to_samples\n",
    "\n",
    "# Optional torch/transformers (guarded)\n",
    "try:\n",
    "    import torch\n",
    "    import torchaudio\n",
    "    from transformers import AutoProcessor, AutoModel\n",
    "    HAS_TORCH = True\n",
    "except Exception:\n",
    "    HAS_TORCH = False\n",
    "\n",
    "sns.set_theme(context=\"notebook\", style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352bdc18",
   "metadata": {},
   "source": [
    "## 2) Configure Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc359ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = (ROOT / 'data').resolve()\n",
    "SAMPLES_DIR = DATA_DIR / 'samples'\n",
    "RECORDINGS_DIR = DATA_DIR / 'recordings'\n",
    "OUTPUT_DIR = (ROOT / 'outputs').resolve()\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Audio parameters\n",
    "TARGET_SR = 16000\n",
    "N_MELS = 64\n",
    "N_MFCC = 13\n",
    "HOP_LENGTH = 160    # 10 ms @ 16 kHz\n",
    "N_FFT = 400         # 25 ms window @ 16 kHz\n",
    "TRIM_DB = 30.0\n",
    "\n",
    "# Ranking\n",
    "TOP_K = 5\n",
    "DTW_METRIC = 'cosine'  # could be 'euclidean'\n",
    "\n",
    "# Embedding model (optional)\n",
    "EMBED_MODEL_ID = 'facebook/wav2vec2-base-960h'  # multilingual alternative: 'facebook/wav2vec2-large-xlsr-53'\n",
    "\n",
    "print('Samples dir:', SAMPLES_DIR)\n",
    "print('Recordings dir:', RECORDINGS_DIR)\n",
    "\n",
    "# Helpers\n",
    "AUDIO_EXTS = {'.wav', '.flac', '.mp3', '.ogg', '.m4a'}\n",
    "\n",
    "def list_audio_files(folder: Path) -> List[Path]:\n",
    "    return [p for p in folder.rglob('*') if p.suffix.lower() in AUDIO_EXTS]\n",
    "\n",
    "def latest_file(folder: Path) -> Path | None:\n",
    "    files = list_audio_files(folder)\n",
    "    if not files:\n",
    "        return None\n",
    "    return max(files, key=lambda p: p.stat().st_mtime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d468d8",
   "metadata": {},
   "source": [
    "## 3) Load Sample Ainu Audio Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_label_from_path(p: Path) -> str:\n",
    "    # e.g., data/samples/<label>/<file>.wav -> use the parent folder as label\n",
    "    try:\n",
    "        return p.parent.name\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "sample_files = list_audio_files(SAMPLES_DIR)\n",
    "if not sample_files:\n",
    "    print(f\"No sample files found in {SAMPLES_DIR}. Please add WAV/FLAC/etc.\")\n",
    "\n",
    "samples_df = pd.DataFrame({\n",
    "    'path': [str(p) for p in sample_files],\n",
    "    'label': [infer_label_from_path(p) for p in sample_files],\n",
    "    'relpath': [str(p.relative_to(SAMPLES_DIR)) for p in sample_files]\n",
    "})\n",
    "print(f\"Indexed {len(samples_df)} sample files.\")\n",
    "samples_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6392aeb4",
   "metadata": {},
   "source": [
    "## 4) Load and Preprocess Recorded Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb657483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a recorded file: either specify manually or take the latest in the recordings folder\n",
    "REC_FILE = latest_file(RECORDINGS_DIR)\n",
    "if REC_FILE is None:\n",
    "    print(f\"No recorded audio found in {RECORDINGS_DIR}. Place a WAV/FLAC/MP3 file there.\")\n",
    "else:\n",
    "    print(\"Using recording:\", REC_FILE)\n",
    "\n",
    "rec_audio = None\n",
    "if REC_FILE is not None:\n",
    "    rec_audio = load_audio(str(REC_FILE), target_sr=TARGET_SR, mono=True)\n",
    "    # Peak normalize to -1 dBFS\n",
    "    peak = np.max(np.abs(rec_audio.y)) if rec_audio.y.size else 0\n",
    "    if peak > 1e-6:\n",
    "        rec_audio = rec_audio.__class__(y=rec_audio.y / peak * 0.89, sr=rec_audio.sr, path=rec_audio.path)\n",
    "    print(f\"Recording duration: {rec_audio.y.shape[0]/rec_audio.sr:.2f} s @ {rec_audio.sr} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14b942",
   "metadata": {},
   "source": [
    "## 5) Voice Activity Detection and Silence Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ee4f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_audio = None\n",
    "if rec_audio is not None:\n",
    "    trimmed_audio = trim_silence(rec_audio, top_db=TRIM_DB)\n",
    "    dur = trimmed_audio.y.shape[0] / trimmed_audio.sr\n",
    "    print(f\"Trimmed duration: {dur:.2f} s (top_db={TRIM_DB})\")\n",
    "\n",
    "# Optional: segment long files into voiced chunks using energy-based VAD\n",
    "# For simplicity, we keep a single trimmed utterance here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab546da7",
   "metadata": {},
   "source": [
    "## 6) Feature Extraction (Log-Mel Spectrograms, MFCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c7ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction for recording\n",
    "F_rec = None\n",
    "if trimmed_audio is not None:\n",
    "    F_rec = extract_features(trimmed_audio, n_mfcc=N_MFCC, hop_length=HOP_LENGTH, n_fft=N_FFT,\n",
    "                             add_deltas=True, add_prosody=True)\n",
    "    print('Recording features:', F_rec.shape)\n",
    "\n",
    "# Feature extraction for samples (on-the-fly, small sets; for large sets, see caching section)\n",
    "sample_feats = []\n",
    "for p in tqdm(sample_files, desc='Extracting sample features'):\n",
    "    try:\n",
    "        a = trim_silence(load_audio(str(p), target_sr=TARGET_SR))\n",
    "        F = extract_features(a, n_mfcc=N_MFCC, hop_length=HOP_LENGTH, n_fft=N_FFT,\n",
    "                             add_deltas=True, add_prosody=True)\n",
    "        sample_feats.append((str(p), F))\n",
    "    except Exception as e:\n",
    "        print('Error processing', p, e)\n",
    "\n",
    "print(f\"Extracted features for {len(sample_feats)} / {len(sample_files)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c41200d",
   "metadata": {},
   "source": [
    "## 7) Feature Normalization and Length Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d9991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if F_rec is not None:\n",
    "    F_rec_z = zscore(F_rec)\n",
    "else:\n",
    "    F_rec_z = None\n",
    "\n",
    "sample_feats_z = []\n",
    "for path, F in sample_feats:\n",
    "    sample_feats_z.append((path, zscore(F)))\n",
    "\n",
    "print('Normalized features. Ready for DTW.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6ce5c0",
   "metadata": {},
   "source": [
    "## 8) DTW-Based Similarity Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2157b637",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_results = []\n",
    "if F_rec_z is not None:\n",
    "    for path, Fz in tqdm(sample_feats_z, desc='DTW scoring'):\n",
    "        try:\n",
    "            dist, _ = dtw_distance(F_rec_z, Fz, metric=DTW_METRIC)\n",
    "            # Normalize by combined length to reduce bias\n",
    "            norm = (F_rec_z.shape[0] + Fz.shape[0])\n",
    "            score = dist / max(norm, 1)\n",
    "            dtw_results.append({\n",
    "                'path': path,\n",
    "                'label': infer_label_from_path(Path(path)),\n",
    "                'dtw_dist': dist,\n",
    "                'dtw_score': score,\n",
    "                'n_frames_sample': Fz.shape[0]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print('DTW failed for', path, e)\n",
    "\n",
    "    dtw_df = pd.DataFrame(dtw_results).sort_values(['dtw_score', 'dtw_dist'], ascending=[True, True])\n",
    "    display(dtw_df.head(TOP_K))\n",
    "else:\n",
    "    dtw_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5b0e97",
   "metadata": {},
   "source": [
    "## 9) Embedding-Based Similarity (Optional, Pretrained Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21af0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_df = pd.DataFrame()\n",
    "if HAS_TORCH:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Embedding device:', device)\n",
    "    try:\n",
    "        processor = AutoProcessor.from_pretrained(EMBED_MODEL_ID)\n",
    "        model = AutoModel.from_pretrained(EMBED_MODEL_ID).to(device).eval()\n",
    "        def embed_audio(y: np.ndarray, sr: int) -> np.ndarray:\n",
    "            # Resample to model's expected input if needed\n",
    "            target_sr = getattr(processor, 'sampling_rate', 16000)\n",
    "            if sr != target_sr:\n",
    "                y_res = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
    "                sr_use = target_sr\n",
    "            else:\n",
    "                y_res = y\n",
    "                sr_use = sr\n",
    "            with torch.no_grad():\n",
    "                inputs = processor(y_res, sampling_rate=sr_use, return_tensors='pt', padding=True)\n",
    "                for k in inputs:\n",
    "                    inputs[k] = inputs[k].to(device)\n",
    "                out = model(**inputs)\n",
    "                # Last hidden state: [B, T, C] -> mean over time\n",
    "                h = out.last_hidden_state.mean(dim=1).squeeze(0).detach().cpu().numpy()\n",
    "                return h\n",
    "        \n",
    "        rec_emb = None\n",
    "        if trimmed_audio is not None:\n",
    "            rec_emb = embed_audio(trimmed_audio.y, trimmed_audio.sr)\n",
    "        emb_rows = []\n",
    "        if rec_emb is not None:\n",
    "            rec_norm = rec_emb / (np.linalg.norm(rec_emb) + 1e-8)\n",
    "            for p in tqdm(sample_files, desc='Embedding scoring'):\n",
    "                try:\n",
    "                    a = load_audio(str(p), target_sr=TARGET_SR)\n",
    "                    e = embed_audio(a.y, a.sr)\n",
    "                    e_norm = e / (np.linalg.norm(e) + 1e-8)\n",
    "                    cos = float(np.dot(rec_norm, e_norm))\n",
    "                    emb_rows.append({'path': str(p), 'label': infer_label_from_path(Path(p)), 'cos_sim': cos})\n",
    "                except Exception as e:\n",
    "                    print('Embedding failed for', p, e)\n",
    "            embed_df = pd.DataFrame(emb_rows).sort_values('cos_sim', ascending=False)\n",
    "            display(embed_df.head(TOP_K))\n",
    "    except Exception as e:\n",
    "        print('Embedding section skipped:', e)\n",
    "else:\n",
    "    print('Torch/transformers not available. Skipping embedding section. To enable, install torch, torchaudio, transformers.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45660fc9",
   "metadata": {},
   "source": [
    "## 10) Ranking and Thresholding of Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f783b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine DTW and embedding (if available). If no embeddings, use DTW only.\n",
    "rank_df = pd.DataFrame()\n",
    "if not dtw_df.empty:\n",
    "    rank_df = dtw_df[['path', 'label', 'dtw_score']].copy()\n",
    "    if not embed_df.empty:\n",
    "        rank_df = rank_df.merge(embed_df[['path', 'cos_sim']], on='path', how='left')\n",
    "        # Convert to a unified score: lower dtw better, higher cos better\n",
    "        # Normalize dtw to [0,1] by min-max over candidates; invert for similarity\n",
    "        dtw_min, dtw_max = rank_df['dtw_score'].min(), rank_df['dtw_score'].max()\n",
    "        if dtw_max > dtw_min:\n",
    "            rank_df['dtw_sim'] = 1.0 - (rank_df['dtw_score'] - dtw_min) / (dtw_max - dtw_min)\n",
    "        else:\n",
    "            rank_df['dtw_sim'] = 1.0\n",
    "        rank_df['cos_sim'] = rank_df['cos_sim'].fillna(rank_df['cos_sim'].min() if not rank_df['cos_sim'].isna().all() else 0.0)\n",
    "        rank_df['score'] = 0.6 * rank_df['dtw_sim'] + 0.4 * rank_df['cos_sim']\n",
    "        rank_df = rank_df.sort_values('score', ascending=False)\n",
    "    else:\n",
    "        # Use 1/dtw as similarity proxy\n",
    "        rank_df['score'] = -rank_df['dtw_score']\n",
    "        rank_df = rank_df.sort_values('score', ascending=False)\n",
    "\n",
    "    display(rank_df.head(TOP_K))\n",
    "else:\n",
    "    print('No DTW results to rank.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df16ddbe",
   "metadata": {},
   "source": [
    "## 11) Visualization of Waveforms, Spectrograms, and DTW Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37368fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display as ipydisplay\n",
    "\n",
    "if rec_audio is not None:\n",
    "    ipydisplay(Audio(rec_audio.y, rate=rec_audio.sr))\n",
    "\n",
    "if F_rec_z is not None and not dtw_df.empty:\n",
    "    top_path = dtw_df.iloc[0]['path']\n",
    "    samp = trim_silence(load_audio(top_path, target_sr=TARGET_SR))\n",
    "    F_samp = extract_features(samp, n_mfcc=N_MFCC, hop_length=HOP_LENGTH, n_fft=N_FFT, add_deltas=True, add_prosody=True)\n",
    "    F_samp_z = zscore(F_samp)\n",
    "\n",
    "    # Waveforms\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 6), constrained_layout=True)\n",
    "    librosa.display.waveshow(trimmed_audio.y, sr=trimmed_audio.sr, ax=axes[0])\n",
    "    axes[0].set_title(f'Recording: {Path(rec_audio.path).name}')\n",
    "    librosa.display.waveshow(samp.y, sr=samp.sr, ax=axes[1], color='orange')\n",
    "    axes[1].set_title(f'Top Sample: {Path(samp.path).name}')\n",
    "    plt.show()\n",
    "\n",
    "    # Similarity matrix and DTW path\n",
    "    cost = 1.0 - cosine_similarity_matrix(F_rec_z, F_samp_z)\n",
    "    dist, path = dtw_distance(F_rec_z, F_samp_z, metric=DTW_METRIC)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,5))\n",
    "    im = ax.imshow(cost, origin='lower', aspect='auto', cmap='magma')\n",
    "    ax.set_title(f'DTW Cost Matrix (dist={dist:.2f})')\n",
    "    ax.set_xlabel('Sample frames')\n",
    "    ax.set_ylabel('Recording frames')\n",
    "    # Plot path\n",
    "    ax.plot(path['index2'], path['index1'], color='cyan', linewidth=1)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aadd99",
   "metadata": {},
   "source": [
    "## 12) Batch Processing and Caching of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193bf717",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = OUTPUT_DIR / 'cache'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_FILE = CACHE_DIR / 'samples_features_mfcc_prosody_npz.npz'\n",
    "\n",
    "if not sample_files:\n",
    "    print('No samples to cache.')\n",
    "else:\n",
    "    if not CACHE_FILE.exists():\n",
    "        print('Caching sample features to', CACHE_FILE)\n",
    "        arrays = {}\n",
    "        meta = {}\n",
    "        for i, (p, F) in enumerate(sample_feats):\n",
    "            key = f'F_{i}'\n",
    "            arrays[key] = F.astype(np.float32)\n",
    "            meta[key] = {'path': p}\n",
    "        np.savez_compressed(CACHE_FILE, **arrays)\n",
    "        with open(CACHE_FILE.with_suffix('.json'), 'w') as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "    else:\n",
    "        print('Cache already exists:', CACHE_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c75db9",
   "metadata": {},
   "source": [
    "## 13) Results Export (CSV/JSON) and Artifact Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b03c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = OUTPUT_DIR / 'results'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not rank_df.empty:\n",
    "    rec_name = Path(rec_audio.path).stem if rec_audio is not None else 'unknown_recording'\n",
    "    out_csv = RESULTS_DIR / f'{rec_name}_top{TOP_K}.csv'\n",
    "    rank_df.head(TOP_K).to_csv(out_csv, index=False)\n",
    "    print('Saved results to', out_csv)\n",
    "\n",
    "    # Save similarity matrix plot for the top result\n",
    "    if not dtw_df.empty:\n",
    "        top_path = dtw_df.iloc[0]['path']\n",
    "        samp = trim_silence(load_audio(top_path, target_sr=TARGET_SR))\n",
    "        F_samp = extract_features(samp, n_mfcc=N_MFCC, hop_length=HOP_LENGTH, n_fft=N_FFT, add_deltas=True, add_prosody=True)\n",
    "        F_samp_z = zscore(F_samp)\n",
    "        cost = 1.0 - cosine_similarity_matrix(F_rec_z, F_samp_z)\n",
    "        fig, ax = plt.subplots(figsize=(6,5))\n",
    "        im = ax.imshow(cost, origin='lower', aspect='auto', cmap='magma')\n",
    "        ax.set_title('Cost matrix')\n",
    "        ax.set_xlabel('Sample frames')\n",
    "        ax.set_ylabel('Recording frames')\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        fig_path = RESULTS_DIR / f'{rec_name}_top1_cost_matrix.png'\n",
    "        fig.savefig(fig_path, dpi=150)\n",
    "        plt.close(fig)\n",
    "        print('Saved plot to', fig_path)\n",
    "else:\n",
    "    print('No ranking to export.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2fb49",
   "metadata": {},
   "source": [
    "## 14) Reusable Functions and Simple Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a3ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as W\n",
    "\n",
    "\n",
    "def run_comparison(recording_path: Path, samples_dir: Path) -> pd.DataFrame:\n",
    "    if not recording_path.exists():\n",
    "        print('Recording not found:', recording_path)\n",
    "        return pd.DataFrame()\n",
    "    sample_paths = list_audio_files(samples_dir)\n",
    "    if not sample_paths:\n",
    "        print('No samples found in', samples_dir)\n",
    "        return pd.DataFrame()\n",
    "    results = compare_recording_to_samples(str(recording_path), [str(p) for p in sample_paths], target_sr=TARGET_SR)\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "rec_opts = ['<latest>'] + [str(p) for p in list_audio_files(RECORDINGS_DIR)]\n",
    "widgets = {\n",
    "    'rec': W.Dropdown(options=rec_opts, description='Recording:'),\n",
    "    'run': W.Button(description='Run', button_style='primary'),\n",
    "    'out': W.Output()\n",
    "}\n",
    "\n",
    "@widgets['run'].on_click\n",
    "def _(_btn):\n",
    "    with widgets['out']:\n",
    "        widgets['out'].clear_output()\n",
    "        sel = widgets['rec'].value\n",
    "        path = latest_file(RECORDINGS_DIR) if sel == '<latest>' else Path(sel)\n",
    "        if path is None:\n",
    "            print('No recording available.')\n",
    "            return\n",
    "        df = run_comparison(path, SAMPLES_DIR)\n",
    "        if not df.empty:\n",
    "            display(df.head(TOP_K))\n",
    "        else:\n",
    "            print('No results.')\n",
    "\n",
    "W.VBox([widgets['rec'], widgets['run'], widgets['out']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbed592",
   "metadata": {},
   "source": [
    "## 15) Basic Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks (safe to run multiple times)\n",
    "if rec_audio is not None:\n",
    "    assert rec_audio.sr == TARGET_SR, f\"Expected SR {TARGET_SR}, got {rec_audio.sr}\"\n",
    "    assert rec_audio.y.ndim == 1, 'Audio should be mono'\n",
    "    assert rec_audio.y.size > 0, 'Audio is empty'\n",
    "\n",
    "if F_rec is not None:\n",
    "    assert F_rec.ndim == 2 and F_rec.shape[0] > 0 and F_rec.shape[1] > 0, 'Invalid feature shape'\n",
    "\n",
    "if trimmed_audio is not None:\n",
    "    voiced_ratio = trimmed_audio.y.size / rec_audio.y.size if rec_audio is not None else 0\n",
    "    print(f\"Voiced ratio after trim: {voiced_ratio:.2f}\")\n",
    "\n",
    "print('Diagnostics complete.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
