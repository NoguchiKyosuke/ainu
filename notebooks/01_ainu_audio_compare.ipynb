{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b47bc615",
   "metadata": {},
   "source": [
    "# Ainu Speech: Recorded Audio vs. Sample Set\n",
    "\n",
    "This notebook analyzes recorded audio and compares it against a sample Ainu audio set using MFCCs, mel features, DTW-based similarity, and optional pretrained speech embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d6cbf3",
   "metadata": {},
   "source": [
    "## 0) Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447bccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running in a fresh environment, uncomment and run this cell once\n",
    "#!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e8c343",
   "metadata": {},
   "source": [
    "## 1) Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fac5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# use seaborn for plotting statistics\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# Local utilities\n",
    "import sys\n",
    "ROOT = Path(\"..\").resolve()\n",
    "sys.path.append(str((ROOT / 'src').resolve()))\n",
    "from audio_utils import load_audio, trim_silence, extract_features, zscore, dtw_distance, cosine_similarity_matrix, compare_recording_to_samples\n",
    "\n",
    "# Optional torch/transformers (guarded)\n",
    "try:\n",
    "    import torch\n",
    "    import torchaudio\n",
    "    from transformers import AutoProcessor, AutoModel\n",
    "    HAS_TORCH = True\n",
    "except Exception:\n",
    "    HAS_TORCH = False\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_theme(context=\"notebook\", style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352bdc18",
   "metadata": {},
   "source": [
    "## 2) Configure Paths and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc359ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = (ROOT / 'data').resolve()\n",
    "SAMPLES_DIR = DATA_DIR / 'samples' / 'vocabulary'\n",
    "RECORDINGS_DIR = DATA_DIR / 'recordings'\n",
    "OUTPUT_DIR = (ROOT / 'outputs').resolve()\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Audio parameters\n",
    "TARGET_SR = 16000\n",
    "N_MELS = 64\n",
    "N_MFCC = 13\n",
    "HOP_LENGTH = 160    # 10 ms @ 16 kHz\n",
    "N_FFT = 400         # 25 ms window @ 16 kHz\n",
    "TRIM_DB = 30.0\n",
    "\n",
    "# Ranking\n",
    "TOP_K = 5\n",
    "DTW_METRIC = 'cosine'  # could be 'euclidean'\n",
    "\n",
    "# Embedding model (optional)\n",
    "EMBED_MODEL_ID = 'facebook/wav2vec2-base-960h'  # multilingual alternative: 'facebook/wav2vec2-large-xlsr-53'\n",
    "\n",
    "print('Samples dir:', SAMPLES_DIR)\n",
    "print('Recordings dir:', RECORDINGS_DIR)\n",
    "\n",
    "# Helpers\n",
    "AUDIO_EXTS = {'.wav', '.flac', '.mp3', '.ogg', '.m4a'}\n",
    "\n",
    "def list_audio_files(folder: Path) -> List[Path]:\n",
    "    # List files without not following symlinks of AUDIO_EXTS\n",
    "    return [p for p in folder.rglob('*') if p.suffix.lower() in AUDIO_EXTS]\n",
    "\n",
    "def latest_file(folder: Path) -> Path | None:\n",
    "    files = list_audio_files(folder)\n",
    "    if not files:\n",
    "        return None\n",
    "    return max(files, key=lambda p: p.stat().st_mtime)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d468d8",
   "metadata": {},
   "source": [
    "## 3) Load Sample Ainu Audio Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555b5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_label_from_path(p: Path) -> str:\n",
    "    # e.g., data/samples/<label>/<file>.wav -> use the parent folder as label\n",
    "    try:\n",
    "        return p.parent.name\n",
    "    except Exception:\n",
    "        return \"unknown\"\n",
    "\n",
    "sample_files = list_audio_files(SAMPLES_DIR)\n",
    "if not sample_files:\n",
    "    print(f\"No sample files found in {SAMPLES_DIR}. Please add WAV/FLAC/etc.\")\n",
    "\n",
    "samples_df = pd.DataFrame({\n",
    "    'path': [str(p) for p in sample_files],\n",
    "    'label': [infer_label_from_path(p) for p in sample_files],\n",
    "    'relpath': [str(p.relative_to(SAMPLES_DIR)) for p in sample_files]\n",
    "})\n",
    "print(f\"Indexed {len(samples_df)} sample files.\")\n",
    "samples_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6392aeb4",
   "metadata": {},
   "source": [
    "## 4) Load and Preprocess Recorded Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb657483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a recorded file: either specify manually or take the latest in the recordings folder\n",
    "REC_FILE = latest_file(RECORDINGS_DIR)\n",
    "if REC_FILE is None:\n",
    "    print(f\"No recorded audio found in {RECORDINGS_DIR}. Place a WAV/FLAC/MP3 file there.\")\n",
    "else:\n",
    "    print(\"Using recording:\", REC_FILE)\n",
    "\n",
    "rec_audio = None\n",
    "if REC_FILE is not None:\n",
    "    rec_audio = load_audio(str(REC_FILE), target_sr=TARGET_SR, mono=True)\n",
    "    # Peak normalize to -1 dBFS\n",
    "    peak = np.max(np.abs(rec_audio.y)) if rec_audio.y.size else 0\n",
    "    if peak > 1e-6:\n",
    "        rec_audio = rec_audio.__class__(y=rec_audio.y / peak * 0.89, sr=rec_audio.sr, path=rec_audio.path)\n",
    "    print(f\"Recording duration: {rec_audio.y.shape[0]/rec_audio.sr:.2f} s @ {rec_audio.sr} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef14b942",
   "metadata": {},
   "source": [
    "## 5) Voice Activity Detection and Silence Trimming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ee4f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "trimmed_audio = None\n",
    "if rec_audio is not None:\n",
    "    # TRIM_DB = 30.0\n",
    "    # trim_silence trims silent sections (margin 0.01 seconds)\n",
    "    trimmed_audio = trim_silence(rec_audio, top_db=TRIM_DB)\n",
    "    dur = trimmed_audio.y.shape[0] / trimmed_audio.sr\n",
    "    print(f\"Trimmed duration: {dur:.2f} s (top_db={TRIM_DB})\")\n",
    "\n",
    "# Optional: segment long files into voiced chunks using energy-based VAD\n",
    "# For simplicity, we keep a single trimmed utterance here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab546da7",
   "metadata": {},
   "source": [
    "## 6) Feature Extraction (Log-Mel Spectrograms, MFCC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c7ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature extraction for recording\n",
    "F_rec = None\n",
    "if trimmed_audio is not None:\n",
    "    # extract_features converts audio to full-body-photo-like representation\n",
    "    #N_MFCC = 13\n",
    "    #HOP_LENGTH = 160    # 10 ms @ 16 kHz\n",
    "    #N_FFT = 400         # 25 ms window @ 16 kHz\n",
    "    F_rec = extract_features(trimmed_audio, n_mfcc=N_MFCC, hop_length=HOP_LENGTH, n_fft=N_FFT, add_deltas=True, add_prosody=True)\n",
    "    print('Recording features:', F_rec.shape)\n",
    "\n",
    "# Feature extraction for samples (on-the-fly, small sets; for large sets, see caching section)\n",
    "sample_feats = []\n",
    "for p in tqdm(sample_files, desc='Extracting sample features'):\n",
    "    try:\n",
    "        a = trim_silence(load_audio(str(p), target_sr=TARGET_SR))\n",
    "        F = extract_features(a, n_mfcc=N_MFCC, hop_length=HOP_LENGTH, n_fft=N_FFT,\n",
    "                             add_deltas=True, add_prosody=True)\n",
    "        sample_feats.append((str(p), F))\n",
    "    except Exception as e:\n",
    "        print('Error processing', p, e)\n",
    "\n",
    "print(f\"Extracted features for {len(sample_feats)} / {len(sample_files)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c41200d",
   "metadata": {},
   "source": [
    "## 7) Feature Normalization and Length Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d9991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if F_rec is not None:\n",
    "    # Normalize features\n",
    "    F_rec_z = zscore(F_rec)\n",
    "else:\n",
    "    F_rec_z = None\n",
    "\n",
    "sample_feats_z = []\n",
    "for path, F in sample_feats:\n",
    "    sample_feats_z.append((path, zscore(F)))\n",
    "\n",
    "print('Normalized features. Ready for DTW.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6ce5c0",
   "metadata": {},
   "source": [
    "## 8) DTW-Based Similarity Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2157b637",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_results = []\n",
    "if F_rec_z is not None:\n",
    "    for path, Fz in tqdm(sample_feats_z, desc='DTW scoring'):\n",
    "        try:\n",
    "            dist, _ = dtw_distance(F_rec_z, Fz, metric=DTW_METRIC)\n",
    "            # Normalize by combined length to reduce bias\n",
    "            norm = (F_rec_z.shape[0] + Fz.shape[0])\n",
    "            score = dist / max(norm, 1)\n",
    "            dtw_results.append({\n",
    "                'path': path,\n",
    "                'label': infer_label_from_path(Path(path)),\n",
    "                'dtw_dist': dist,\n",
    "                'dtw_score': score,\n",
    "                'n_frames_sample': Fz.shape[0]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print('DTW failed for', path, e)\n",
    "\n",
    "    dtw_df = pd.DataFrame(dtw_results).sort_values(['dtw_score', 'dtw_dist'], ascending=[True, True])\n",
    "    display(dtw_df.head(TOP_K))\n",
    "else:\n",
    "    dtw_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5b0e97",
   "metadata": {},
   "source": [
    "## 9) Embedding-Based Similarity (Optional, Pretrained Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21af0279",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_df = pd.DataFrame()\n",
    "if HAS_TORCH:\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print('Embedding device:', device)\n",
    "    try:\n",
    "        processor = AutoProcessor.from_pretrained(EMBED_MODEL_ID)\n",
    "        model = AutoModel.from_pretrained(EMBED_MODEL_ID).to(device).eval()\n",
    "        def embed_audio(y: np.ndarray, sr: int) -> np.ndarray:\n",
    "            # Resample to model's expected input if needed\n",
    "            target_sr = getattr(processor, 'sampling_rate', 16000)\n",
    "            if sr != target_sr:\n",
    "                y_res = librosa.resample(y, orig_sr=sr, target_sr=target_sr)\n",
    "                sr_use = target_sr\n",
    "            else:\n",
    "                y_res = y\n",
    "                sr_use = sr\n",
    "            with torch.no_grad():\n",
    "                inputs = processor(y_res, sampling_rate=sr_use, return_tensors='pt', padding=True)\n",
    "                for k in inputs:\n",
    "                    inputs[k] = inputs[k].to(device)\n",
    "                out = model(**inputs)\n",
    "                # Last hidden state: [B, T, C] -> mean over time\n",
    "                h = out.last_hidden_state.mean(dim=1).squeeze(0).detach().cpu().numpy()\n",
    "                return h\n",
    "        \n",
    "        rec_emb = None\n",
    "        if trimmed_audio is not None:\n",
    "            rec_emb = embed_audio(trimmed_audio.y, trimmed_audio.sr)\n",
    "        emb_rows = []\n",
    "        if rec_emb is not None:\n",
    "            rec_norm = rec_emb / (np.linalg.norm(rec_emb) + 1e-8)\n",
    "            for p in tqdm(sample_files, desc='Embedding scoring'):\n",
    "                try:\n",
    "                    a = load_audio(str(p), target_sr=TARGET_SR)\n",
    "                    e = embed_audio(a.y, a.sr)\n",
    "                    e_norm = e / (np.linalg.norm(e) + 1e-8)\n",
    "                    cos = float(np.dot(rec_norm, e_norm))\n",
    "                    emb_rows.append({'path': str(p), 'label': infer_label_from_path(Path(p)), 'cos_sim': cos})\n",
    "                except Exception as e:\n",
    "                    print('Embedding failed for', p, e)\n",
    "            embed_df = pd.DataFrame(emb_rows).sort_values('cos_sim', ascending=False)\n",
    "            display(embed_df.head(TOP_K))\n",
    "    except Exception as e:\n",
    "        print('Embedding section skipped:', e)\n",
    "else:\n",
    "    print('Torch/transformers not available. Skipping embedding section. To enable, install torch, torchaudio, transformers.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45660fc9",
   "metadata": {},
   "source": [
    "## 10) Ranking and Thresholding of Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f783b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine DTW and embedding (if available). If no embeddings, use DTW only.\n",
    "rank_df = pd.DataFrame()\n",
    "if not dtw_df.empty:\n",
    "    rank_df = dtw_df[['path', 'label', 'dtw_score']].copy()\n",
    "    if not embed_df.empty:\n",
    "        rank_df = rank_df.merge(embed_df[['path', 'cos_sim']], on='path', how='left')\n",
    "        # Convert to a unified score: lower dtw better, higher cos better\n",
    "        # Normalize dtw to [0,1] by min-max over candidates; invert for similarity\n",
    "        dtw_min, dtw_max = rank_df['dtw_score'].min(), rank_df['dtw_score'].max()\n",
    "        if dtw_max > dtw_min:\n",
    "            rank_df['dtw_sim'] = 1.0 - (rank_df['dtw_score'] - dtw_min) / (dtw_max - dtw_min)\n",
    "        else:\n",
    "            rank_df['dtw_sim'] = 1.0\n",
    "        rank_df['cos_sim'] = rank_df['cos_sim'].fillna(rank_df['cos_sim'].min() if not rank_df['cos_sim'].isna().all() else 0.0)\n",
    "        rank_df['score'] = 0.6 * rank_df['dtw_sim'] + 0.4 * rank_df['cos_sim']\n",
    "        rank_df = rank_df.sort_values('score', ascending=False)\n",
    "    else:\n",
    "        # Use 1/dtw as similarity proxy\n",
    "        rank_df['score'] = -rank_df['dtw_score']\n",
    "        rank_df = rank_df.sort_values('score', ascending=False)\n",
    "\n",
    "    display(rank_df.head(TOP_K))\n",
    "else:\n",
    "    print('No DTW results to rank.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df16ddbe",
   "metadata": {},
   "source": [
    "## 11) Visualization of Waveforms, Spectrograms, and DTW Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37368fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display as ipydisplay\n",
    "\n",
    "if rec_audio is not None:\n",
    "    ipydisplay(Audio(rec_audio.y, rate=rec_audio.sr))\n",
    "\n",
    "if F_rec_z is not None and not dtw_df.empty:\n",
    "    top_path = dtw_df.iloc[0]['path']\n",
    "    samp = trim_silence(load_audio(top_path, target_sr=TARGET_SR))\n",
    "    F_samp = extract_features(samp, n_mfcc=N_MFCC, hop_length=HOP_LENGTH, n_fft=N_FFT, add_deltas=True, add_prosody=True)\n",
    "    F_samp_z = zscore(F_samp)\n",
    "\n",
    "    # Waveforms\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 6), constrained_layout=True)\n",
    "    librosa.display.waveshow(trimmed_audio.y, sr=trimmed_audio.sr, ax=axes[0])\n",
    "    axes[0].set_title(f'Recording: {Path(rec_audio.path).name}')\n",
    "    librosa.display.waveshow(samp.y, sr=samp.sr, ax=axes[1], color='orange')\n",
    "    axes[1].set_title(f'Top Sample: {Path(samp.path).name}')\n",
    "    plt.show()\n",
    "\n",
    "    # Similarity matrix and DTW path\n",
    "    cost = 1.0 - cosine_similarity_matrix(F_rec_z, F_samp_z)\n",
    "    dist, path = dtw_distance(F_rec_z, F_samp_z, metric=DTW_METRIC)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6,5))\n",
    "    im = ax.imshow(cost, origin='lower', aspect='auto', cmap='magma')\n",
    "    ax.set_title(f'DTW Cost Matrix (dist={dist:.2f})')\n",
    "    ax.set_xlabel('Sample frames')\n",
    "    ax.set_ylabel('Recording frames')\n",
    "    # Plot path\n",
    "    ax.plot(path['index2'], path['index1'], color='cyan', linewidth=1)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aadd99",
   "metadata": {},
   "source": [
    "## 12) Batch Processing and Caching of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193bf717",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = OUTPUT_DIR / 'cache'\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_FILE = CACHE_DIR / 'samples_features_mfcc_prosody_npz.npz'\n",
    "\n",
    "if not sample_files:\n",
    "    print('No samples to cache.')\n",
    "else:\n",
    "    if not CACHE_FILE.exists():\n",
    "        print('Caching sample features to', CACHE_FILE)\n",
    "        arrays = {}\n",
    "        meta = {}\n",
    "        for i, (p, F) in enumerate(sample_feats):\n",
    "            key = f'F_{i}'\n",
    "            arrays[key] = F.astype(np.float32)\n",
    "            meta[key] = {'path': p}\n",
    "        np.savez_compressed(CACHE_FILE, **arrays)\n",
    "        with open(CACHE_FILE.with_suffix('.json'), 'w') as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "    else:\n",
    "        print('Cache already exists:', CACHE_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c75db9",
   "metadata": {},
   "source": [
    "## 13) Results Export (CSV/JSON) and Artifact Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b03c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_DIR = OUTPUT_DIR / 'results'\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if not rank_df.empty:\n",
    "    rec_name = Path(rec_audio.path).stem if rec_audio is not None else 'unknown_recording'\n",
    "    out_csv = RESULTS_DIR / f'{rec_name}_top{TOP_K}.csv'\n",
    "    rank_df.head(TOP_K).to_csv(out_csv, index=False)\n",
    "    print('Saved results to', out_csv)\n",
    "\n",
    "    # Save similarity matrix plot for the top result\n",
    "    if not dtw_df.empty:\n",
    "        top_path = dtw_df.iloc[0]['path']\n",
    "        samp = trim_silence(load_audio(top_path, target_sr=TARGET_SR))\n",
    "        F_samp = extract_features(samp, n_mfcc=N_MFCC, hop_length=HOP_LENGTH, n_fft=N_FFT, add_deltas=True, add_prosody=True)\n",
    "        F_samp_z = zscore(F_samp)\n",
    "        cost = 1.0 - cosine_similarity_matrix(F_rec_z, F_samp_z)\n",
    "        fig, ax = plt.subplots(figsize=(6,5))\n",
    "        im = ax.imshow(cost, origin='lower', aspect='auto', cmap='magma')\n",
    "        ax.set_title('Cost matrix')\n",
    "        ax.set_xlabel('Sample frames')\n",
    "        ax.set_ylabel('Recording frames')\n",
    "        fig.colorbar(im, ax=ax)\n",
    "        fig_path = RESULTS_DIR / f'{rec_name}_top1_cost_matrix.png'\n",
    "        fig.savefig(fig_path, dpi=150)\n",
    "        plt.close(fig)\n",
    "        print('Saved plot to', fig_path)\n",
    "else:\n",
    "    print('No ranking to export.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c2fb49",
   "metadata": {},
   "source": [
    "## 14) Reusable Functions and Simple Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a3ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as W\n",
    "\n",
    "\n",
    "def run_comparison(recording_path: Path, samples_dir: Path) -> pd.DataFrame:\n",
    "    if not recording_path.exists():\n",
    "        print('Recording not found:', recording_path)\n",
    "        return pd.DataFrame()\n",
    "    sample_paths = list_audio_files(samples_dir)\n",
    "    if not sample_paths:\n",
    "        print('No samples found in', samples_dir)\n",
    "        return pd.DataFrame()\n",
    "    results = compare_recording_to_samples(str(recording_path), [str(p) for p in sample_paths], target_sr=TARGET_SR)\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "rec_opts = ['<latest>'] + [str(p) for p in list_audio_files(RECORDINGS_DIR)]\n",
    "widgets = {\n",
    "    'rec': W.Dropdown(options=rec_opts, description='Recording:'),\n",
    "    'run': W.Button(description='Run', button_style='primary'),\n",
    "    'out': W.Output()\n",
    "}\n",
    "\n",
    "@widgets['run'].on_click\n",
    "def _(_btn):\n",
    "    with widgets['out']:\n",
    "        widgets['out'].clear_output()\n",
    "        sel = widgets['rec'].value\n",
    "        path = latest_file(RECORDINGS_DIR) if sel == '<latest>' else Path(sel)\n",
    "        if path is None:\n",
    "            print('No recording available.')\n",
    "            return\n",
    "        df = run_comparison(path, SAMPLES_DIR)\n",
    "        if not df.empty:\n",
    "            display(df.head(TOP_K))\n",
    "        else:\n",
    "            print('No results.')\n",
    "\n",
    "W.VBox([widgets['rec'], widgets['run'], widgets['out']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbed592",
   "metadata": {},
   "source": [
    "## 15) Basic Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b7eed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks (safe to run multiple times)\n",
    "if rec_audio is not None:\n",
    "    assert rec_audio.sr == TARGET_SR, f\"Expected SR {TARGET_SR}, got {rec_audio.sr}\"\n",
    "    assert rec_audio.y.ndim == 1, 'Audio should be mono'\n",
    "    assert rec_audio.y.size > 0, 'Audio is empty'\n",
    "\n",
    "if F_rec is not None:\n",
    "    assert F_rec.ndim == 2 and F_rec.shape[0] > 0 and F_rec.shape[1] > 0, 'Invalid feature shape'\n",
    "\n",
    "if trimmed_audio is not None:\n",
    "    voiced_ratio = trimmed_audio.y.size / rec_audio.y.size if rec_audio is not None else 0\n",
    "    print(f\"Voiced ratio after trim: {voiced_ratio:.2f}\")\n",
    "\n",
    "print('Diagnostics complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254fb975",
   "metadata": {},
   "source": [
    "## 15) Direct File Comparison Function\n",
    "\n",
    "This section provides functionality to directly compare two specific audio files, useful for targeted pronunciation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf021cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_specific_files(recording_path: str, sample_path: str, word: str = \"unknown\"):\n",
    "    \"\"\"\n",
    "    Compare two specific audio files directly\n",
    "    \n",
    "    Args:\n",
    "        recording_path: Path to recorded audio file\n",
    "        sample_path: Path to reference sample file  \n",
    "        word: Word being pronounced (for labeling)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with comparison results\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    \n",
    "    print(f\"🎵 Ainu Audio Analysis: Comparing '{word}' pronunciations\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Recording: {recording_path}\")\n",
    "    print(f\"Sample:    {sample_path}\")\n",
    "    print()\n",
    "    \n",
    "    # Check if files exist\n",
    "    if not Path(recording_path).exists():\n",
    "        print(f\"❌ Recording file not found: {recording_path}\")\n",
    "        return None\n",
    "    if not Path(sample_path).exists():\n",
    "        print(f\"❌ Sample file not found: {sample_path}\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Load recording\n",
    "        print(\"📂 Loading recording...\")\n",
    "        rec_audio = load_audio(recording_path, target_sr=TARGET_SR, mono=True)\n",
    "        rec_duration = rec_audio.y.shape[0] / rec_audio.sr\n",
    "        print(f\"   Duration: {rec_duration:.2f}s @ {rec_audio.sr} Hz\")\n",
    "        \n",
    "        # Load sample\n",
    "        print(\"📂 Loading sample...\")\n",
    "        samp_audio = load_audio(sample_path, target_sr=TARGET_SR, mono=True)\n",
    "        samp_duration = samp_audio.y.shape[0] / samp_audio.sr\n",
    "        print(f\"   Duration: {samp_duration:.2f}s @ {samp_audio.sr} Hz\")\n",
    "        print()\n",
    "        \n",
    "        # Trim silence\n",
    "        print(\"✂️  Trimming silence...\")\n",
    "        rec_trimmed = trim_silence(rec_audio, top_db=TRIM_DB)\n",
    "        samp_trimmed = trim_silence(samp_audio, top_db=TRIM_DB)\n",
    "        \n",
    "        rec_trim_dur = rec_trimmed.y.shape[0] / rec_trimmed.sr\n",
    "        samp_trim_dur = samp_trimmed.y.shape[0] / samp_trimmed.sr\n",
    "        print(f\"   Recording trimmed: {rec_trim_dur:.2f}s\")\n",
    "        print(f\"   Sample trimmed:    {samp_trim_dur:.2f}s\")\n",
    "        print()\n",
    "        \n",
    "        # Extract features\n",
    "        print(\"🔬 Extracting features...\")\n",
    "        rec_features = extract_features(rec_trimmed, n_mfcc=N_MFCC, hop_length=HOP_LENGTH, \n",
    "                                      n_fft=N_FFT, add_deltas=True, add_prosody=True)\n",
    "        samp_features = extract_features(samp_trimmed, n_mfcc=N_MFCC, hop_length=HOP_LENGTH,\n",
    "                                       n_fft=N_FFT, add_deltas=True, add_prosody=True)\n",
    "        \n",
    "        print(f\"   Recording features: {rec_features.shape} (frames × features)\")\n",
    "        print(f\"   Sample features:    {samp_features.shape} (frames × features)\")\n",
    "        print()\n",
    "        \n",
    "        # Normalize features\n",
    "        print(\"📊 Normalizing features...\")\n",
    "        rec_norm = zscore(rec_features)\n",
    "        samp_norm = zscore(samp_features)\n",
    "        print()\n",
    "        \n",
    "        # Calculate DTW similarity\n",
    "        print(\"🧮 Calculating DTW similarity...\")\n",
    "        try:\n",
    "            dtw_result = dtw_distance(rec_norm, samp_norm, metric=DTW_METRIC)\n",
    "            if isinstance(dtw_result, tuple) and len(dtw_result) == 2:\n",
    "                dtw_dist, dtw_path = dtw_result\n",
    "            else:\n",
    "                dtw_dist = dtw_result\n",
    "                dtw_path = None\n",
    "        except:\n",
    "            # Fallback DTW calculation\n",
    "            cost_matrix = 1.0 - cosine_similarity_matrix(rec_norm, samp_norm)\n",
    "            from dtw import dtw\n",
    "            alignment = dtw(cost_matrix)\n",
    "            dtw_dist = alignment.distance\n",
    "            dtw_path = list(zip(alignment.index1, alignment.index2))\n",
    "        \n",
    "        # Normalize DTW score by sequence lengths\n",
    "        total_frames = rec_norm.shape[0] + samp_norm.shape[0]\n",
    "        dtw_score = dtw_dist / max(total_frames, 1)\n",
    "        \n",
    "        print(f\"   DTW distance: {dtw_dist:.6f}\")\n",
    "        print(f\"   DTW score (normalized): {dtw_score:.8f}\")\n",
    "        print()\n",
    "        \n",
    "        # Calculate frame-wise cosine similarity\n",
    "        print(\"📈 Calculating cosine similarity matrix...\")\n",
    "        cos_sim_matrix = cosine_similarity_matrix(rec_norm, samp_norm)\n",
    "        mean_cos_sim = np.mean(cos_sim_matrix)\n",
    "        max_cos_sim = np.max(cos_sim_matrix)\n",
    "        \n",
    "        print(f\"   Mean cosine similarity: {mean_cos_sim:.4f}\")\n",
    "        print(f\"   Max cosine similarity:  {max_cos_sim:.4f}\")\n",
    "        print()\n",
    "        \n",
    "        # Analysis results\n",
    "        print(\"🎯 ANALYSIS RESULTS\")\n",
    "        print(\"=\" * 30)\n",
    "        print(f\"Word:                 '{word}'\")\n",
    "        print(f\"Recording duration:   {rec_trim_dur:.2f}s\")\n",
    "        print(f\"Sample duration:      {samp_trim_dur:.2f}s\")\n",
    "        print(f\"DTW alignment score:  {dtw_score:.8f} (lower = more similar)\")\n",
    "        print(f\"Cosine similarity:    {mean_cos_sim:.4f} (higher = more similar)\")\n",
    "        \n",
    "        # Interpretation\n",
    "        print()\n",
    "        print(\"📝 INTERPRETATION\")\n",
    "        print(\"=\" * 20)\n",
    "        if dtw_score < 0.001:\n",
    "            similarity_level = \"EXCELLENT\"\n",
    "        elif dtw_score < 0.01:\n",
    "            similarity_level = \"VERY GOOD\"\n",
    "        elif dtw_score < 0.1:\n",
    "            similarity_level = \"GOOD\"\n",
    "        elif dtw_score < 1.0:\n",
    "            similarity_level = \"MODERATE\"\n",
    "        else:\n",
    "            similarity_level = \"LOW\"\n",
    "            \n",
    "        print(f\"Pronunciation similarity: {similarity_level}\")\n",
    "        \n",
    "        if mean_cos_sim > 0.8:\n",
    "            acoustic_match = \"EXCELLENT acoustic match\"\n",
    "        elif mean_cos_sim > 0.6:\n",
    "            acoustic_match = \"GOOD acoustic match\"\n",
    "        elif mean_cos_sim > 0.4:\n",
    "            acoustic_match = \"MODERATE acoustic match\"\n",
    "        else:\n",
    "            acoustic_match = \"LOW acoustic match\"\n",
    "            \n",
    "        print(f\"Acoustic characteristics: {acoustic_match}\")\n",
    "        \n",
    "        # Return results dictionary\n",
    "        return {\n",
    "            'recording_file': recording_path,\n",
    "            'sample_file': sample_path,\n",
    "            'word': word,\n",
    "            'recording_duration': rec_trim_dur,\n",
    "            'sample_duration': samp_trim_dur,\n",
    "            'dtw_distance': dtw_dist,\n",
    "            'dtw_score': dtw_score,\n",
    "            'mean_cosine_similarity': mean_cos_sim,\n",
    "            'max_cosine_similarity': max_cos_sim,\n",
    "            'similarity_level': similarity_level,\n",
    "            'acoustic_match': acoustic_match,\n",
    "            'features': {\n",
    "                'recording': rec_norm,\n",
    "                'sample': samp_norm\n",
    "            },\n",
    "            'audio': {\n",
    "                'recording': rec_trimmed,\n",
    "                'sample': samp_trimmed\n",
    "            },\n",
    "            'cos_sim_matrix': cos_sim_matrix,\n",
    "            'dtw_path': dtw_path\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during analysis: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"✅ Direct comparison function loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b0652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_comparison(result_dict, save_path=None):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for audio comparison results\n",
    "    \n",
    "    Args:\n",
    "        result_dict: Results from compare_specific_files()\n",
    "        save_path: Optional path to save the visualization\n",
    "    \"\"\"\n",
    "    if result_dict is None:\n",
    "        print(\"❌ No results to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Extract data\n",
    "    rec_audio = result_dict['audio']['recording']\n",
    "    samp_audio = result_dict['audio']['sample']\n",
    "    rec_features = result_dict['features']['recording']\n",
    "    samp_features = result_dict['features']['sample']\n",
    "    cos_sim_matrix = result_dict['cos_sim_matrix']\n",
    "    word = result_dict['word']\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(16, 12))\n",
    "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    # 1. Recording waveform\n",
    "    ax1 = fig.add_subplot(gs[0, 0])\n",
    "    time_rec = np.linspace(0, len(rec_audio.y) / rec_audio.sr, len(rec_audio.y))\n",
    "    ax1.plot(time_rec, rec_audio.y, color='blue', alpha=0.7, linewidth=0.8)\n",
    "    ax1.set_title(f\"Recording: '{word}'\", fontsize=12, fontweight='bold')\n",
    "    ax1.set_xlabel('Time (s)')\n",
    "    ax1.set_ylabel('Amplitude')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Sample waveform\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    time_samp = np.linspace(0, len(samp_audio.y) / samp_audio.sr, len(samp_audio.y))\n",
    "    ax2.plot(time_samp, samp_audio.y, color='red', alpha=0.7, linewidth=0.8)\n",
    "    ax2.set_title(f\"Reference Sample: '{word}'\", fontsize=12, fontweight='bold')\n",
    "    ax2.set_xlabel('Time (s)')\n",
    "    ax2.set_ylabel('Amplitude')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Spectrograms\n",
    "    ax3 = fig.add_subplot(gs[1, 0])\n",
    "    D_rec = librosa.amplitude_to_db(np.abs(librosa.stft(rec_audio.y, hop_length=HOP_LENGTH, n_fft=N_FFT)), ref=np.max)\n",
    "    img1 = librosa.display.specshow(D_rec, sr=rec_audio.sr, hop_length=HOP_LENGTH, \n",
    "                                   x_axis='time', y_axis='hz', ax=ax3, cmap='viridis')\n",
    "    ax3.set_title('Recording Spectrogram', fontsize=11)\n",
    "    ax3.set_ylabel('Frequency (Hz)')\n",
    "    \n",
    "    ax4 = fig.add_subplot(gs[1, 1])\n",
    "    D_samp = librosa.amplitude_to_db(np.abs(librosa.stft(samp_audio.y, hop_length=HOP_LENGTH, n_fft=N_FFT)), ref=np.max)\n",
    "    img2 = librosa.display.specshow(D_samp, sr=samp_audio.sr, hop_length=HOP_LENGTH,\n",
    "                                   x_axis='time', y_axis='hz', ax=ax4, cmap='viridis')\n",
    "    ax4.set_title('Sample Spectrogram', fontsize=11)\n",
    "    ax4.set_ylabel('Frequency (Hz)')\n",
    "    \n",
    "    # 4. MFCC comparison\n",
    "    ax5 = fig.add_subplot(gs[1, 2])\n",
    "    mfcc_rec = rec_features[:, :N_MFCC]  # First N_MFCC coefficients\n",
    "    mfcc_samp = samp_features[:, :N_MFCC]\n",
    "    \n",
    "    # Plot MFCC coefficients\n",
    "    time_frames_rec = np.arange(mfcc_rec.shape[0])\n",
    "    time_frames_samp = np.arange(mfcc_samp.shape[0])\n",
    "    \n",
    "    ax5.plot(time_frames_rec, np.mean(mfcc_rec, axis=1), 'b-', label='Recording', alpha=0.8)\n",
    "    ax5.plot(time_frames_samp, np.mean(mfcc_samp, axis=1), 'r-', label='Sample', alpha=0.8)\n",
    "    ax5.set_title('MFCC Comparison (Mean)', fontsize=11)\n",
    "    ax5.set_xlabel('Frame')\n",
    "    ax5.set_ylabel('MFCC Value')\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Cosine similarity matrix\n",
    "    ax6 = fig.add_subplot(gs[2, 0])\n",
    "    im = ax6.imshow(cos_sim_matrix, cmap='RdYlBu_r', aspect='auto', origin='lower')\n",
    "    ax6.set_title('Cosine Similarity Matrix', fontsize=11)\n",
    "    ax6.set_xlabel('Sample Frames')\n",
    "    ax6.set_ylabel('Recording Frames')\n",
    "    plt.colorbar(im, ax=ax6, shrink=0.8)\n",
    "    \n",
    "    # 6. DTW path visualization (if available)\n",
    "    ax7 = fig.add_subplot(gs[2, 1])\n",
    "    if result_dict.get('dtw_path'):\n",
    "        dtw_path = result_dict['dtw_path']\n",
    "        if dtw_path and len(dtw_path) > 0:\n",
    "            path_x, path_y = zip(*dtw_path)\n",
    "            ax7.imshow(cos_sim_matrix, cmap='RdYlBu_r', aspect='auto', origin='lower', alpha=0.6)\n",
    "            ax7.plot(path_y, path_x, 'w-', linewidth=2, alpha=0.8)\n",
    "            ax7.set_title('DTW Alignment Path', fontsize=11)\n",
    "            ax7.set_xlabel('Sample Frames')\n",
    "            ax7.set_ylabel('Recording Frames')\n",
    "        else:\n",
    "            ax7.text(0.5, 0.5, 'DTW Path\\nNot Available', ha='center', va='center', \n",
    "                    transform=ax7.transAxes, fontsize=12)\n",
    "            ax7.set_title('DTW Alignment Path', fontsize=11)\n",
    "    else:\n",
    "        ax7.text(0.5, 0.5, 'DTW Path\\nNot Available', ha='center', va='center', \n",
    "                transform=ax7.transAxes, fontsize=12)\n",
    "        ax7.set_title('DTW Alignment Path', fontsize=11)\n",
    "    \n",
    "    # 7. Summary statistics\n",
    "    ax8 = fig.add_subplot(gs[2, 2])\n",
    "    ax8.axis('off')\n",
    "    \n",
    "    # Create summary text\n",
    "    summary_text = f\"\"\"\n",
    "COMPARISON SUMMARY\n",
    "{'='*20}\n",
    "\n",
    "Word: '{word}'\n",
    "\n",
    "Durations:\n",
    "  Recording: {result_dict['recording_duration']:.2f}s\n",
    "  Sample: {result_dict['sample_duration']:.2f}s\n",
    "\n",
    "Similarity Metrics:\n",
    "  DTW Score: {result_dict['dtw_score']:.6f}\n",
    "  Cosine Sim: {result_dict['mean_cosine_similarity']:.4f}\n",
    "  Max Cosine: {result_dict['max_cosine_similarity']:.4f}\n",
    "\n",
    "Assessment:\n",
    "  Level: {result_dict['similarity_level']}\n",
    "  Match: {result_dict['acoustic_match']}\n",
    "    \"\"\"\n",
    "    \n",
    "    ax8.text(0.05, 0.95, summary_text, transform=ax8.transAxes, fontsize=10,\n",
    "            verticalalignment='top', fontfamily='monospace',\n",
    "            bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))\n",
    "    \n",
    "    # Overall title\n",
    "    fig.suptitle(f\"Ainu Pronunciation Analysis: '{word}'\", fontsize=16, fontweight='bold', y=0.98)\n",
    "    \n",
    "    # Save if requested\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"📊 Visualization saved to: {save_path}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✅ Visualization function loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931cf0b1",
   "metadata": {},
   "source": [
    "## 16) Example: Compare \"manu\" pronunciations\n",
    "\n",
    "Now let's use our new functions to compare specific files. We'll analyze the \"manu\" (flower) pronunciation that we found in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368bad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths for \"manu\" comparison\n",
    "recording_file = \"data/recordings/manu.wav\"\n",
    "sample_file = \"data/samples/Raven_and_the_First_Men/manu.wav\"\n",
    "\n",
    "# Check if files exist before proceeding\n",
    "from pathlib import Path\n",
    "\n",
    "if Path(recording_file).exists() and Path(sample_file).exists():\n",
    "    print(\"🔍 Files found! Proceeding with comparison...\")\n",
    "    \n",
    "    # Run the comparison\n",
    "    manu_results = compare_specific_files(recording_file, sample_file, word=\"manu\")\n",
    "    \n",
    "    if manu_results:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"📊 GENERATING VISUALIZATION...\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Create visualization\n",
    "        visualize_comparison(manu_results, save_path=\"outputs/results/manu_comparison.png\")\n",
    "        \n",
    "        # Save detailed results to CSV\n",
    "        import pandas as pd\n",
    "        \n",
    "        # Create results summary\n",
    "        results_summary = {\n",
    "            'word': [manu_results['word']],\n",
    "            'recording_file': [manu_results['recording_file']],\n",
    "            'sample_file': [manu_results['sample_file']],\n",
    "            'recording_duration': [manu_results['recording_duration']],\n",
    "            'sample_duration': [manu_results['sample_duration']],\n",
    "            'dtw_distance': [manu_results['dtw_distance']],\n",
    "            'dtw_score': [manu_results['dtw_score']],\n",
    "            'mean_cosine_similarity': [manu_results['mean_cosine_similarity']],\n",
    "            'max_cosine_similarity': [manu_results['max_cosine_similarity']],\n",
    "            'similarity_level': [manu_results['similarity_level']],\n",
    "            'acoustic_match': [manu_results['acoustic_match']]\n",
    "        }\n",
    "        \n",
    "        df_results = pd.DataFrame(results_summary)\n",
    "        results_path = \"outputs/results/manu_detailed_results.csv\"\n",
    "        df_results.to_csv(results_path, index=False)\n",
    "        print(f\"💾 Detailed results saved to: {results_path}\")\n",
    "        \n",
    "        print(\"\\n🎉 Analysis complete!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Comparison failed!\")\n",
    "        \n",
    "else:\n",
    "    print(\"⚠️  One or both files not found:\")\n",
    "    print(f\"   Recording: {recording_file} {'✅' if Path(recording_file).exists() else '❌'}\")\n",
    "    print(f\"   Sample:    {sample_file} {'✅' if Path(sample_file).exists() else '❌'}\")\n",
    "    print()\n",
    "    print(\"📋 Available sample files with 'manu':\")\n",
    "    \n",
    "    # Search for manu files in samples directory\n",
    "    samples_dir = Path(\"data/samples\")\n",
    "    if samples_dir.exists():\n",
    "        manu_files = list(samples_dir.rglob(\"*manu*\"))\n",
    "        if manu_files:\n",
    "            for i, f in enumerate(manu_files, 1):\n",
    "                print(f\"   {i}. {f}\")\n",
    "        else:\n",
    "            print(\"   No files containing 'manu' found\")\n",
    "    else:\n",
    "        print(\"   Samples directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f934dc1",
   "metadata": {},
   "source": [
    "## 17) Advanced Phonetic Analysis\n",
    "\n",
    "For deeper linguistic analysis, let's add functions that examine specific phonetic characteristics like formants, pitch patterns, and spectral features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116cb7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_phonetic_features(audio_data, sr, n_fft=2048, hop_length=512):\n",
    "    \"\"\"\n",
    "    Extract detailed phonetic features for linguistic analysis\n",
    "    \n",
    "    Args:\n",
    "        audio_data: Audio time series\n",
    "        sr: Sample rate\n",
    "        n_fft: FFT window size\n",
    "        hop_length: Hop length for analysis\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with phonetic analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Basic spectral features\n",
    "    spectral_centroids = librosa.feature.spectral_centroid(y=audio_data, sr=sr, hop_length=hop_length)[0]\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio_data, sr=sr, hop_length=hop_length)[0]\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio_data, sr=sr, hop_length=hop_length)[0]\n",
    "    zero_crossing_rate = librosa.feature.zero_crossing_rate(audio_data, hop_length=hop_length)[0]\n",
    "    \n",
    "    # Pitch (F0) analysis\n",
    "    f0 = librosa.yin(audio_data, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'), sr=sr)\n",
    "    \n",
    "    # Mel-frequency features\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=sr, n_fft=n_fft, hop_length=hop_length)\n",
    "    mel_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
    "    \n",
    "    # Chroma features (useful for tonal analysis)\n",
    "    chroma = librosa.feature.chroma_stft(y=audio_data, sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    # Temporal features\n",
    "    onset_frames = librosa.onset.onset_detect(y=audio_data, sr=sr, hop_length=hop_length)\n",
    "    tempo, beats = librosa.beat.beat_track(y=audio_data, sr=sr, hop_length=hop_length)\n",
    "    \n",
    "    # Energy analysis\n",
    "    rms_energy = librosa.feature.rms(y=audio_data, hop_length=hop_length)[0]\n",
    "    \n",
    "    return {\n",
    "        'spectral_centroid': {\n",
    "            'mean': np.mean(spectral_centroids),\n",
    "            'std': np.std(spectral_centroids),\n",
    "            'values': spectral_centroids\n",
    "        },\n",
    "        'spectral_rolloff': {\n",
    "            'mean': np.mean(spectral_rolloff),\n",
    "            'std': np.std(spectral_rolloff),\n",
    "            'values': spectral_rolloff\n",
    "        },\n",
    "        'spectral_bandwidth': {\n",
    "            'mean': np.mean(spectral_bandwidth),\n",
    "            'std': np.std(spectral_bandwidth),\n",
    "            'values': spectral_bandwidth\n",
    "        },\n",
    "        'zero_crossing_rate': {\n",
    "            'mean': np.mean(zero_crossing_rate),\n",
    "            'std': np.std(zero_crossing_rate),\n",
    "            'values': zero_crossing_rate\n",
    "        },\n",
    "        'f0_pitch': {\n",
    "            'mean': np.mean(f0[f0 > 0]),  # Exclude zero values\n",
    "            'std': np.std(f0[f0 > 0]),\n",
    "            'min': np.min(f0[f0 > 0]) if np.sum(f0 > 0) > 0 else 0,\n",
    "            'max': np.max(f0[f0 > 0]) if np.sum(f0 > 0) > 0 else 0,\n",
    "            'values': f0\n",
    "        },\n",
    "        'rms_energy': {\n",
    "            'mean': np.mean(rms_energy),\n",
    "            'std': np.std(rms_energy),\n",
    "            'values': rms_energy\n",
    "        },\n",
    "        'mel_spectrogram': mel_db,\n",
    "        'chroma': chroma,\n",
    "        'onset_frames': onset_frames,\n",
    "        'tempo': tempo,\n",
    "        'beat_frames': beats\n",
    "    }\n",
    "\n",
    "def detailed_phonetic_comparison(result_dict):\n",
    "    \"\"\"\n",
    "    Perform detailed phonetic comparison between recording and sample\n",
    "    \n",
    "    Args:\n",
    "        result_dict: Results from compare_specific_files()\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with phonetic comparison results\n",
    "    \"\"\"\n",
    "    if result_dict is None:\n",
    "        return None\n",
    "    \n",
    "    # Extract audio data\n",
    "    rec_audio = result_dict['audio']['recording']\n",
    "    samp_audio = result_dict['audio']['sample']\n",
    "    \n",
    "    print(\"🔬 Analyzing phonetic features...\")\n",
    "    \n",
    "    # Analyze both recordings\n",
    "    rec_phonetic = analyze_phonetic_features(rec_audio.y, rec_audio.sr)\n",
    "    samp_phonetic = analyze_phonetic_features(samp_audio.y, samp_audio.sr)\n",
    "    \n",
    "    # Compare key features\n",
    "    comparisons = {}\n",
    "    \n",
    "    # Spectral comparisons\n",
    "    for feature in ['spectral_centroid', 'spectral_rolloff', 'spectral_bandwidth', 'zero_crossing_rate', 'rms_energy']:\n",
    "        rec_mean = rec_phonetic[feature]['mean']\n",
    "        samp_mean = samp_phonetic[feature]['mean']\n",
    "        \n",
    "        # Calculate relative difference\n",
    "        if samp_mean != 0:\n",
    "            relative_diff = abs(rec_mean - samp_mean) / samp_mean * 100\n",
    "        else:\n",
    "            relative_diff = 0\n",
    "            \n",
    "        comparisons[feature] = {\n",
    "            'recording_mean': rec_mean,\n",
    "            'sample_mean': samp_mean,\n",
    "            'absolute_difference': abs(rec_mean - samp_mean),\n",
    "            'relative_difference_percent': relative_diff,\n",
    "            'similarity_score': max(0, 100 - relative_diff)  # Simple similarity score\n",
    "        }\n",
    "    \n",
    "    # Pitch comparison (F0)\n",
    "    rec_f0_mean = rec_phonetic['f0_pitch']['mean']\n",
    "    samp_f0_mean = samp_phonetic['f0_pitch']['mean']\n",
    "    \n",
    "    if not (np.isnan(rec_f0_mean) or np.isnan(samp_f0_mean)) and samp_f0_mean > 0:\n",
    "        f0_relative_diff = abs(rec_f0_mean - samp_f0_mean) / samp_f0_mean * 100\n",
    "        f0_similarity = max(0, 100 - f0_relative_diff)\n",
    "    else:\n",
    "        f0_relative_diff = 0\n",
    "        f0_similarity = 0\n",
    "    \n",
    "    comparisons['f0_pitch'] = {\n",
    "        'recording_mean': rec_f0_mean,\n",
    "        'sample_mean': samp_f0_mean,\n",
    "        'absolute_difference': abs(rec_f0_mean - samp_f0_mean) if not (np.isnan(rec_f0_mean) or np.isnan(samp_f0_mean)) else 0,\n",
    "        'relative_difference_percent': f0_relative_diff,\n",
    "        'similarity_score': f0_similarity\n",
    "    }\n",
    "    \n",
    "    # Overall phonetic similarity (weighted average)\n",
    "    weights = {\n",
    "        'spectral_centroid': 0.25,\n",
    "        'spectral_rolloff': 0.20,\n",
    "        'spectral_bandwidth': 0.15,\n",
    "        'zero_crossing_rate': 0.10,\n",
    "        'f0_pitch': 0.20,\n",
    "        'rms_energy': 0.10\n",
    "    }\n",
    "    \n",
    "    overall_similarity = sum(comparisons[feature]['similarity_score'] * weights[feature] \n",
    "                           for feature in weights.keys())\n",
    "    \n",
    "    print(f\"📊 Phonetic analysis complete!\")\n",
    "    print(f\"   Overall phonetic similarity: {overall_similarity:.1f}%\")\n",
    "    \n",
    "    return {\n",
    "        'recording_features': rec_phonetic,\n",
    "        'sample_features': samp_phonetic,\n",
    "        'feature_comparisons': comparisons,\n",
    "        'overall_phonetic_similarity': overall_similarity,\n",
    "        'similarity_weights': weights\n",
    "    }\n",
    "\n",
    "def create_phonetic_report(result_dict, phonetic_analysis, save_path=None):\n",
    "    \"\"\"\n",
    "    Generate a detailed phonetic analysis report\n",
    "    \"\"\"\n",
    "    if result_dict is None or phonetic_analysis is None:\n",
    "        return\n",
    "        \n",
    "    word = result_dict['word']\n",
    "    \n",
    "    report = f\"\"\"\n",
    "# Detailed Phonetic Analysis Report: '{word}'\n",
    "\n",
    "## Overview\n",
    "- **Word**: {word}\n",
    "- **Recording Duration**: {result_dict['recording_duration']:.2f}s\n",
    "- **Sample Duration**: {result_dict['sample_duration']:.2f}s\n",
    "- **Overall Phonetic Similarity**: {phonetic_analysis['overall_phonetic_similarity']:.1f}%\n",
    "\n",
    "## Feature Comparison Summary\n",
    "\n",
    "| Feature | Recording | Sample | Difference | Similarity |\n",
    "|---------|-----------|---------|------------|------------|\n",
    "\"\"\"\n",
    "    \n",
    "    for feature, comparison in phonetic_analysis['feature_comparisons'].items():\n",
    "        feature_name = feature.replace('_', ' ').title()\n",
    "        rec_val = comparison['recording_mean']\n",
    "        samp_val = comparison['sample_mean']\n",
    "        diff_pct = comparison['relative_difference_percent']\n",
    "        sim_score = comparison['similarity_score']\n",
    "        \n",
    "        if feature == 'f0_pitch':\n",
    "            report += f\"| {feature_name} | {rec_val:.1f} Hz | {samp_val:.1f} Hz | {diff_pct:.1f}% | {sim_score:.1f}% |\\n\"\n",
    "        else:\n",
    "            report += f\"| {feature_name} | {rec_val:.3f} | {samp_val:.3f} | {diff_pct:.1f}% | {sim_score:.1f}% |\\n\"\n",
    "    \n",
    "    report += f\"\"\"\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "### Spectral Characteristics\n",
    "- **Spectral Centroid**: Indicates the \"brightness\" of the sound\n",
    "- **Spectral Rolloff**: Shows the frequency below which 85% of energy is contained\n",
    "- **Spectral Bandwidth**: Measures the width of the frequency spectrum\n",
    "\n",
    "### Temporal Characteristics  \n",
    "- **Zero Crossing Rate**: Related to the \"roughness\" or \"noisiness\" of the signal\n",
    "- **RMS Energy**: Overall loudness/intensity of the speech\n",
    "\n",
    "### Pitch Analysis\n",
    "- **F0 (Fundamental Frequency)**: Primary pitch of the voice\n",
    "\n",
    "### Overall Assessment\n",
    "Based on the phonetic analysis, the pronunciation similarity is **{phonetic_analysis['overall_phonetic_similarity']:.1f}%**.\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add interpretation based on similarity score\n",
    "    if phonetic_analysis['overall_phonetic_similarity'] >= 85:\n",
    "        assessment = \"EXCELLENT - Very similar phonetic characteristics\"\n",
    "    elif phonetic_analysis['overall_phonetic_similarity'] >= 70:\n",
    "        assessment = \"GOOD - Generally similar with minor differences\"\n",
    "    elif phonetic_analysis['overall_phonetic_similarity'] >= 55:\n",
    "        assessment = \"MODERATE - Some similarities but noticeable differences\"\n",
    "    else:\n",
    "        assessment = \"NEEDS IMPROVEMENT - Significant phonetic differences\"\n",
    "    \n",
    "    report += f\"**Assessment**: {assessment}\\n\"\n",
    "    \n",
    "    if save_path:\n",
    "        with open(save_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(report)\n",
    "        print(f\"📝 Phonetic report saved to: {save_path}\")\n",
    "    \n",
    "    print(\"📋 PHONETIC ANALYSIS REPORT\")\n",
    "    print(\"=\" * 50)\n",
    "    print(report)\n",
    "    \n",
    "    return report\n",
    "\n",
    "print(\"✅ Advanced phonetic analysis functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2175aa",
   "metadata": {},
   "source": [
    "## 18) Interactive Analysis Widget\n",
    "\n",
    "Let's create an interactive widget that allows easy selection and comparison of audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4902fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import ipywidgets as widgets\n",
    "    from IPython.display import display, HTML, Audio\n",
    "    \n",
    "    def create_interactive_analyzer():\n",
    "        \"\"\"\n",
    "        Create an interactive widget for audio file comparison\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get available files\n",
    "        from pathlib import Path\n",
    "        \n",
    "        recordings_dir = Path(\"data/recordings\")\n",
    "        samples_dir = Path(\"data/samples\")\n",
    "        \n",
    "        # Find recording files\n",
    "        recording_files = []\n",
    "        if recordings_dir.exists():\n",
    "            recording_files = [str(f.relative_to(recordings_dir.parent)) for f in recordings_dir.glob(\"*.wav\")]\n",
    "        \n",
    "        # Find sample files  \n",
    "        sample_files = []\n",
    "        if samples_dir.exists():\n",
    "            sample_files = [str(f.relative_to(samples_dir.parent)) for f in samples_dir.rglob(\"*.wav\")]\n",
    "        \n",
    "        # Create widgets\n",
    "        recording_dropdown = widgets.Dropdown(\n",
    "            options=recording_files,\n",
    "            value=recording_files[0] if recording_files else None,\n",
    "            description='Recording:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout={'width': '400px'}\n",
    "        )\n",
    "        \n",
    "        sample_dropdown = widgets.Dropdown(\n",
    "            options=sample_files,\n",
    "            value=sample_files[0] if sample_files else None,\n",
    "            description='Sample:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout={'width': '400px'}\n",
    "        )\n",
    "        \n",
    "        word_input = widgets.Text(\n",
    "            value='unknown',\n",
    "            description='Word:',\n",
    "            style={'description_width': 'initial'},\n",
    "            layout={'width': '200px'}\n",
    "        )\n",
    "        \n",
    "        analyze_button = widgets.Button(\n",
    "            description='🔬 Analyze Pronunciation',\n",
    "            button_style='info',\n",
    "            layout={'width': '200px'}\n",
    "        )\n",
    "        \n",
    "        detailed_button = widgets.Button(\n",
    "            description='📊 Detailed Analysis',\n",
    "            button_style='success', \n",
    "            layout={'width': '200px'}\n",
    "        )\n",
    "        \n",
    "        output_area = widgets.Output()\n",
    "        \n",
    "        def on_analyze_click(b):\n",
    "            with output_area:\n",
    "                output_area.clear_output()\n",
    "                \n",
    "                if not recording_dropdown.value or not sample_dropdown.value:\n",
    "                    print(\"❌ Please select both recording and sample files\")\n",
    "                    return\n",
    "                \n",
    "                print(\"🚀 Starting analysis...\")\n",
    "                \n",
    "                # Run comparison\n",
    "                results = compare_specific_files(\n",
    "                    recording_dropdown.value,\n",
    "                    sample_dropdown.value, \n",
    "                    word_input.value\n",
    "                )\n",
    "                \n",
    "                if results:\n",
    "                    print(\"\\\\n📊 Creating visualization...\")\n",
    "                    visualize_comparison(results)\n",
    "                    \n",
    "                    # Show audio players for comparison\n",
    "                    print(\"\\\\n🎵 Audio Players:\")\n",
    "                    print(\"Recording:\")\n",
    "                    display(Audio(recording_dropdown.value))\n",
    "                    print(\"Sample:\")\n",
    "                    display(Audio(sample_dropdown.value))\n",
    "        \n",
    "        def on_detailed_click(b):\n",
    "            with output_area:\n",
    "                output_area.clear_output()\n",
    "                \n",
    "                if not recording_dropdown.value or not sample_dropdown.value:\n",
    "                    print(\"❌ Please select both recording and sample files\")\n",
    "                    return\n",
    "                \n",
    "                print(\"🔬 Starting detailed phonetic analysis...\")\n",
    "                \n",
    "                # Run basic comparison first\n",
    "                results = compare_specific_files(\n",
    "                    recording_dropdown.value,\n",
    "                    sample_dropdown.value,\n",
    "                    word_input.value\n",
    "                )\n",
    "                \n",
    "                if results:\n",
    "                    # Run detailed phonetic analysis\n",
    "                    phonetic_results = detailed_phonetic_comparison(results)\n",
    "                    \n",
    "                    if phonetic_results:\n",
    "                        # Create report\n",
    "                        report_path = f\"outputs/results/{word_input.value}_phonetic_report.md\"\n",
    "                        create_phonetic_report(results, phonetic_results, save_path=report_path)\n",
    "                        \n",
    "                        # Show visualization\n",
    "                        print(\"\\\\n📊 Creating visualization...\")\n",
    "                        visualize_comparison(results)\n",
    "        \n",
    "        # Connect button events\n",
    "        analyze_button.on_click(on_analyze_click)\n",
    "        detailed_button.on_click(on_detailed_click)\n",
    "        \n",
    "        # Layout\n",
    "        controls = widgets.VBox([\n",
    "            widgets.HTML(\"<h3>🎵 Ainu Pronunciation Analyzer</h3>\"),\n",
    "            widgets.HBox([recording_dropdown, word_input]),\n",
    "            sample_dropdown,\n",
    "            widgets.HBox([analyze_button, detailed_button]),\n",
    "            widgets.HTML(\"<hr>\")\n",
    "        ])\n",
    "        \n",
    "        return widgets.VBox([controls, output_area])\n",
    "    \n",
    "    # Create and display the widget\n",
    "    analyzer_widget = create_interactive_analyzer()\n",
    "    display(analyzer_widget)\n",
    "    \n",
    "    print(\"✅ Interactive analyzer ready!\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠️  ipywidgets not available. Using manual approach instead.\")\n",
    "    print(\"📋 Available files for manual comparison:\")\n",
    "    \n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Show available files\n",
    "    recordings_dir = Path(\"data/recordings\")\n",
    "    samples_dir = Path(\"data/samples\")\n",
    "    \n",
    "    print(\"\\\\n📁 Recording files:\")\n",
    "    if recordings_dir.exists():\n",
    "        recording_files = list(recordings_dir.glob(\"*.wav\"))\n",
    "        for i, f in enumerate(recording_files, 1):\n",
    "            print(f\"   {i}. {f}\")\n",
    "    else:\n",
    "        print(\"   No recordings directory found\")\n",
    "    \n",
    "    print(\"\\\\n📁 Sample files (showing first 10):\")\n",
    "    if samples_dir.exists():\n",
    "        sample_files = list(samples_dir.rglob(\"*.wav\"))\n",
    "        for i, f in enumerate(sample_files[:10], 1):\n",
    "            print(f\"   {i}. {f}\")\n",
    "        if len(sample_files) > 10:\n",
    "            print(f\"   ... and {len(sample_files) - 10} more files\")\n",
    "    else:\n",
    "        print(\"   No samples directory found\")\n",
    "    \n",
    "    print(\"\\\\n💡 To run comparison manually, use:\")\n",
    "    print(\"   results = compare_specific_files('path/to/recording.wav', 'path/to/sample.wav', 'word')\")\n",
    "    print(\"   visualize_comparison(results)\")\n",
    "    print(\"   phonetic_analysis = detailed_phonetic_comparison(results)\")\n",
    "    print(\"   create_phonetic_report(results, phonetic_analysis)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a36c8d",
   "metadata": {},
   "source": [
    "## 🎉 Notebook Conversion Complete!\n",
    "\n",
    "This notebook now contains all the functionality from the standalone Python scripts:\n",
    "\n",
    "### 📋 What's Available:\n",
    "\n",
    "1. **Basic Analysis Functions** (Sections 1-14)\n",
    "   - Audio loading, preprocessing, and feature extraction\n",
    "   - DTW distance calculation and cosine similarity\n",
    "   - Visualization and batch processing capabilities\n",
    "\n",
    "2. **Direct File Comparison** (Sections 15-16)\n",
    "   - `compare_specific_files()` - Complete pronunciation analysis\n",
    "   - `visualize_comparison()` - Comprehensive visualization suite\n",
    "   - Example usage with \"manu\" files\n",
    "\n",
    "3. **Advanced Phonetic Analysis** (Section 17)\n",
    "   - `analyze_phonetic_features()` - Detailed spectral and pitch analysis\n",
    "   - `detailed_phonetic_comparison()` - Feature-by-feature comparison\n",
    "   - `create_phonetic_report()` - Formatted analysis reports\n",
    "\n",
    "4. **Interactive Interface** (Section 18)\n",
    "   - Widget-based file selection (if ipywidgets available)\n",
    "   - Integrated audio playback\n",
    "   - One-click analysis and reporting\n",
    "\n",
    "### 🚀 Quick Start Examples:\n",
    "\n",
    "```python\n",
    "# Basic comparison\n",
    "results = compare_specific_files('data/recordings/manu.wav', 'data/samples/story/manu.wav', 'manu')\n",
    "visualize_comparison(results)\n",
    "\n",
    "# Detailed phonetic analysis  \n",
    "phonetic_analysis = detailed_phonetic_comparison(results)\n",
    "create_phonetic_report(results, phonetic_analysis, 'my_analysis_report.md')\n",
    "```\n",
    "\n",
    "### 📁 Output Files:\n",
    "- Visualizations: `outputs/results/`\n",
    "- CSV results: `outputs/results/`\n",
    "- Phonetic reports: `outputs/results/`\n",
    "- Cached features: `outputs/cache/`\n",
    "\n",
    "The notebook workflow now provides the same functionality as the standalone scripts with improved interactivity and visualization!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
